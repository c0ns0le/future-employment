# Survey Suggestions

## Outreach Email
* Be specific about:
	* Length of the survey
	* What types of questions
	* Intended purpose of the rankings
* Play on the Oxford name
* Receive endorsement from the mailing list owner

## Introduction

* The current wording is not clear. Make it super simple.
* Assume that people will read a random number of sentences half-way through perfectly.
* Be specific about:
	* Length of the survey
	* What types of questions
	* Intended purpose of the rankings
* Explain clearly that any personal information won't be shared/manipulated/used – clarify wording of this through Oxford ethics board
* Be careful not to anchor the respondent's answers with the examples at the beginning.

## Demographic Information

* Capture email or text at the end
* Don't "invite" extra text (to sift through). Encourage them to respond by text ONLY if something **really** needs to be said.
* "Postgraduate student" is a little confusing.
* Order experience options by degree/intensity.

## Survey – Construction

* What incentive can we offer?
	* **Monetary:**
		* Charity donation $X / survey up to $Y
	* **Non-Monetary:**
		* Appeal to the self-importance gained from filling out the survey
		* Explain the societal benefits / "helping the world" factor
		* Provide survey results immediately
		* Follow up with expert interviews
* Adaptive question sampling would enlarge our training set.
	* Set a threshold confidence interval. With each answer, recalculate confidence intervals for questions. Cycle out ones within the interval and replace them with new questions.

## Survey – Sampling

* We may have serious non-response bias through lists. We can mitigate this through double sampling.

## Survey Questions & Answers

* At the top of the answers page, remind respondents that we're just interested in what is __CURRENTLY__ automatable.
* How do we crisply define "Automation"?
	* If automation automated away 9/10 people, is that "Automatable" or "with some simplification"?
	* Do we mean "computerisable" (much clearer)?
	* Technically, one might believe that __anything__ is automatable with enough data. Make it very clear that we're just interested in whether or not a specific task can be automated right now.
* 30 questions is small. It may not be a very useful training set. If we have many hundreds of people.
* Have "calibration" questions to measure individual response bias.
* They would like guidance on when to say "unsure"
* For that matter, create specific "unsure" options, like:
	* Unsure – suspect automatable, don't know the technology
	* Unsure – no experience/qualification/knowledge
* It is worth curating the question list. Some occupations may be too wild to answer and some may be too obvious.
* Make all questions & answers required.
* We may want to include a question at the _end_ that asks: __"Overall, how confident were you in the answers you gave about which tasks are currently automatable?"__
* __New__ questions that were proposed:
	* Ask a "10-years-from-now" variant for each occupation
	* Ask specifically about component skills
	* Ask about what _will_ be automated – i.e. it can, and it's __cost-effective__

## Meta

* Run multiple pilots in house.
* Run a "2-Stage Pilot" using "Cognitive Task Interviews"
	* __Stage 1:__ Interactive observation. Ask surveyees questions about their experience as they go
	* __Stage 2:__ Passive observation. Observe and ask for feedback after.
* It would be worth speaking with an economist about acceptable survey methodology and responses.