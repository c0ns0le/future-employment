{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import GPy\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Logan/Google Drive/Oxford/DPhil/future_employment/data/helpers/skills\n"
     ]
    }
   ],
   "source": [
    "cd ../../../data/helpers/skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('skills_2009.csv')\n",
    "Y = pd.read_csv('automation_targets.csv')\n",
    "codes = pd.read_csv('codes_index.csv')\n",
    "y = pd.read_csv('automation_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## UNSOLVED\n",
    "# Using time series data â€“ multiple years\n",
    "\n",
    "## NOTES ABOUT GP MODELS\n",
    "# optimize hyperparameters\n",
    "# perform random restarts\n",
    "# allow for multiple kernels\n",
    "# optimize kernels\n",
    "# optimize variances\n",
    "\n",
    "## TO DO\n",
    "# percentile regression model evaluation\n",
    "\n",
    "# CREATE REGRESSION MODELS\n",
    "from GPy.models import GPRegression\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from GPy.models import GPClassification\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_models = [('gpreg', GPRegression),\n",
    "\t\t\t  ('bayes_ridge', BayesianRidge()),\n",
    "\t\t\t  ('Gboost', GradientBoostingRegressor()),\n",
    "\t\t\t  ('support_vec_reg', SVR())\n",
    "\t\t\t ]\n",
    "\n",
    "# CREATE CLASSIFICATION MODELS\n",
    "class_models = [('gpclass', GPClassification),\n",
    "\t\t\t\t('ridge_class', RidgeClassifier()),\n",
    "\t\t\t\t('gbclass', GradientBoostingClassifier()),\n",
    "\t\t\t\t('support_vec_class', SVC()),\n",
    "\t\t\t\t('naivebayes', BernoulliNB())\n",
    "\t\t \t\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CREATE INPUTS LIST\n",
    "def get_array_percentiles(array):\n",
    "\t\tdef percentile(x, array):\n",
    "\t\t\treturn 100*np.mean(array <= x)\n",
    "\n",
    "\t\treturn np.array(map(lambda x: percentile(x, array), array))\n",
    "\n",
    "def get_percentiles(df):\n",
    "\tif isinstance(df, pd.DataFrame):\n",
    "\t\tnew_df = df.copy()\n",
    "\t\treturn new_df.apply(lambda x: get_array_percentiles(x), axis = 0)\n",
    "\telif isinstance(df, np.ndarray):\n",
    "\t\treturn np.apply_along_axis(get_percentiles, 0, a)\n",
    "\telse:\n",
    "\t\tprint \"TYPE ERROR; PLEASE INPUT pd.DataFrame OR np.ndarray\"\n",
    "\t\traise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"skills_2009.csv\")\n",
    "X_skills, X_task, X_context = X, X, X\n",
    "# X_skills = pd.read_csv(\"X_skills.csv\")\n",
    "# X_task = pd.read_csv(\"X_task.csv\")\n",
    "# X_context = pd.read_csv(\"X_context.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = pd.read_csv(\"automation_targets.csv\")\n",
    "Y.columns = [\"O*NET-SOC Code\", \"auto_15\", \"auto_9\", \"delta\", \"auto_delta_pct\", \"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "codes = pd.read_csv(\"codes_index.csv\")\n",
    "full_X = pd.concat((X_skills, codes), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_X = full_X.merge(Y, on = \"O*NET-SOC Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for dX in [full_X]:\n",
    "    pctiles = get_percentiles(dX)\n",
    "    d_list =  [('dx', dX), ('pctile', pctiles)]\n",
    "    for d in d_list:\n",
    "        prefix = d[0]\n",
    "        data = d[1]\n",
    "        inputs.append((prefix, data.iloc[:,:-6]))\n",
    "        inputs.append((prefix + '_greater_than', data[data.auto_delta_pct > 0].iloc[:,:-6]))\n",
    "        inputs.append((prefix + '_less_than', data[data.auto_delta_pct < 0].iloc[:,:-6]))\n",
    "# \t\tinputs.append(data[emp_delta_pct > 0])\n",
    "# \t\tinputs.append(data[emp_delta_pct < 0])\n",
    "\n",
    "# new_inputs = []\n",
    "# for inp in inputs:\n",
    "#     new_inputs.append(inp[0], inp[1].iloc[:,:-6])\n",
    "\n",
    "# inputs = new_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "automation = pd.read_csv(\"automation_targets.csv\")\n",
    "codes = automation['O*NET-SOC Code']\n",
    "automation = automation.iloc[:, [0,2,1]]\n",
    "full_X = full_X.drop([\"auto_15\", \"auto_9\", \"delta\", \"auto_delta_pct\", \"title\"], axis = 1)\n",
    "temp_X = full_X.merge(automation, how = 'inner', on = 'O*NET-SOC Code')\n",
    "y_out = temp_X[['automation_9', 'automation_15']]\n",
    "codes = temp_X[\"O*NET-SOC Code\"]\n",
    "full_X = temp_X.drop(['O*NET-SOC Code', 'automation_9', 'automation_15'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# automation = pd.read_csv(\"y_automation.csv\")\n",
    "# computerisation = pd.read_csv(\"y_computerisation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CREATE CLASS TARGETS\n",
    "class_targets = []\n",
    "pos_threshold_values = [0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.00, 2.25, 2.50, 2.75, 3.00]\n",
    "neg_threshold_values = map(lambda x: -x, pos_threshold_values)\n",
    "threshold_values = pos_threshold_values + neg_threshold_values\n",
    "\n",
    "for name, target in reg_targets:\n",
    "    for sd_threshold_value in threshold_values:\n",
    "        low_threshold = target > (np.mean(target) - sd_threshold_value*np.std(target))\n",
    "        new_name = str(sd_threshold_value)+\"_gt_\" + name\n",
    "        class_targets.append((new_name,low_threshold))\n",
    "\n",
    "# \tmean_threshold = target > np.mean(target)\n",
    "# \tnew_name = 'mean_gt_' + name\n",
    "# \tclass_targets.append((name, mean_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_inputs = inputs\n",
    "y_inputs_reg, y_inputs_class = reg_targets, class_targets\n",
    "models_reg, models_class = reg_models, class_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### aggregate models as list of tuples [(name, val), (name, val)]\n",
    "# loop\n",
    "\n",
    "def run_loop(X_inputs, y_inputs_reg, y_inputs_class, models_reg, models_class):\n",
    "    ## returns all models in comparison, as dict\n",
    "    evals = defaultdict(defaultdict({}))\n",
    "    for X_inp in X_inputs: #FOR ALL INPUTS\n",
    "        X_name, X = X_inputs[0], X_inputs[1]\n",
    "\n",
    "        for pred_type in ['classification', 'regression']:\n",
    "            if pred_type == 'regression':\n",
    "                y_inps = y_inputs_reg\n",
    "                mods = models_reg\n",
    "            elif pred_type == 'classification':\n",
    "                y_inputs = y_inputs_class\n",
    "                mods = models_class\n",
    "            else:\n",
    "                return \"SOMETHING WRONG!\"\n",
    "        # do regression\n",
    "        for y_inp in y_inps: #FOR ALL TARGETS\n",
    "            # format outcome data\n",
    "            y_name, y = y_inp[0], y_inp[1]\n",
    "            if y.ndim == 1:\n",
    "                y = y[:, np.newaxis]\n",
    "            \n",
    "            #format feature matrix data\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "            data = [X_train, X_test, y_train, y_test]\n",
    "            new_data = []\n",
    "            # check that all data is np.ndarry\n",
    "            for d in data:\n",
    "                if not isinstance(data, np.ndarray):\n",
    "                    new_data.append(np.array(data))\n",
    "                else:\n",
    "                    new_data.append(data)\n",
    "            X_train, X_test, y_train, y_test = new_data\n",
    "\n",
    "            #loop over models\n",
    "            for mod in mods: #FOR APPROPRIATE MODELS\n",
    "                model_name, model = models[0], models[1]\n",
    "                try:\n",
    "                    model.fit(X_train, y_train)\n",
    "                except AttributeError:\n",
    "                    model = model(X_train, y_train) # THESE ARE GPs!\n",
    "                    model.optimize()\n",
    "                    model.optimize_restarts(20)\n",
    "                y_pred = model.predict(X_test)\n",
    "                score = score_model(y_pred, y_test, X_inp, mod, score_type = pred_type) # CREATE TYPES\n",
    "                evals[y_name][model_name][X_name] = score\n",
    "            \n",
    "            # do classification\n",
    "\treturn evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_ARD(model, feature_names):\n",
    "\t# uses ARD to find relevant features\n",
    "\tlength_scales = model.length_scales # np.ndarray\n",
    "\tfeatures = range(1, len(model.features) + 1)\n",
    "\timportances = 1./length_scales\n",
    "\tcutoff = 0.25 * min(importances) # note: arbitrary\n",
    "\timportant_features = features * (importances >= cutoff)\n",
    "\tif_indices = np.trim_zeros(important_features)\n",
    "\treturn zip(feature_names[if_indices], importances[if_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_model(y_pred, y_test, model_inputs, model, score_type):\n",
    "\tscore = {}\n",
    "\tX_name, X = model_inputs[0], model_inputs[1]\n",
    "\tmodel_name, model = models[0], models[1]\n",
    "\n",
    "\tif score_type == \"classification\":\n",
    "\t\taccuracy = -1\n",
    "\t\tprecision = None\n",
    "\t\trecall = None\n",
    "\t\tspecificity = None\n",
    "\t\tf1 = None\n",
    "\t\tideal_cutoff = None\n",
    "\n",
    "\t\tchart_storage = np.array([None, None, None])\n",
    "\t\tfor cutoff in np.arange(0, 1.001, 0.001):\n",
    "\t\t\ty_pred = y_pred > cutoff\n",
    "\t\t\tnum_TP = float(np.sum((y_pred == 1) & (y_test == 1)))\n",
    "\t\t\tnum_FP = float(np.sum((y_pred == 1) & (y_test == 0)))\n",
    "\t\t\tnum_TN = float(np.sum((y_pred == 0) & (y_test == 0)))\n",
    "\t\t\tnum_FN = float(np.sum((y_pred == 0) & (y_test == 1)))\n",
    "\n",
    "\t\t\ts_accuracy = np.mean(y_pred == y_test)\n",
    "\t\t\ts_precision = num_TP / (num_TP + num_FP)\n",
    "\t\t\ts_recall = num_TP / (num_TP + num_FN)\n",
    "\t\t\ts_specificity = num_TN / (num_TN + num_FP)\n",
    "\t\t\ts_f1 = (precision * recall) / (precision + recall)\n",
    "\n",
    "\t\t\tchart_storage.append([cutoff, s_recall, s_specificity])\n",
    "\n",
    "\t\t\tif s_accuracy > accuracy:\n",
    "\t\t\t\tideal_cutoff = cutoff\n",
    "\t\t\t\tprecision = s_precision\n",
    "\t\t\t\trecall = s_recall\n",
    "\t\t\t\tspecificity = s_specificity \n",
    "\t\t\t\tf1 = s_f1\n",
    "\t\t\t\tideal_cutoff = s_ideal_cutoff\n",
    "\n",
    "\t\tdef plot_chart(chart_storage):\n",
    "\t\t\tcutoffs = chart_storage[:,0]\n",
    "\t\t\tsensitivity = chart_storage[:,1]\n",
    "\t\t\tspecificity = chart_storage[:,2]\n",
    "\t\t\tsns.plt.plot(sensitivity, specificity)\n",
    "\t\t\tsns.plt.close()\n",
    "\n",
    "\t\teval_text = \"\"\"\n",
    "\t\tModel \t\t| \t{}\t\t|\n",
    "\t\tX name\t\t|\t{}\t\t|\n",
    "\t\tAccuracy\t|\t{}\t\t|\n",
    "\t\tPrecision  \t|\t{}\t\t|\n",
    "\t\tRecall \t\t|\t{}\t\t|\n",
    "\t\tSpecificity |\t{}\t\t|\n",
    "\t\tF1\t\t\t|\t{}\t\t|\n",
    "\t\t\"\"\".format(model_name,\n",
    "\t\t\t\t   X_name,\n",
    "\t\t\t\t   accuracy,\n",
    "\t\t\t\t   precision,\n",
    "\t\t\t\t   recalll,\n",
    "\t\t\t\t   specificity,\n",
    "\t\t\t\t   f1)\n",
    "\t\tscore['Model name'] = model_name\n",
    "\t\tscore['Model'] = model\n",
    "\t\tscore['X_name'] = X_name\n",
    "\t\tscore['Accuracy'] = accuracy\n",
    "\t\tscore['Precision'] = precision\n",
    "\t\tscore['Recall'] = recall\n",
    "\t\tscore['Specificity'] = specificity\n",
    "\t\tscore['F1'] = F1\n",
    "\t\tscore['AUC_data'] = chart_storage\n",
    "\t\tscore['chart'] = plot_chart # when called, call on plot_chart( score['AUC_data'])\n",
    "\n",
    "\t\tprint eval_text\n",
    "\t\treturn score\n",
    "\n",
    "\telif score_type == 'regression':\n",
    "\t\terrors = y_test - y_pred\n",
    "\t\tse = float(np.std(errors)) / np.sqrt(len(errors))\n",
    "\n",
    "\t\teval_text = \"\"\"\n",
    "\t\tModel \t\t| \t{}\t\t|\n",
    "\t\tX name\t\t|\t{}\t\t|\n",
    "\t\tMean Y\t\t|\t{}\t\t|\n",
    "\t\tMean Y_hat\t|\t{}\t\t|\n",
    "\t\tMean error\t|\t{}\t\t|\n",
    "\t\tSE\t\t\t| \t{}\t\t|\n",
    "\t\tLikelihood\t|\t{}\t\t|\n",
    "\t\t\"\"\".format(model_name,\n",
    "\t\t\t\t   X_name,\n",
    "\t\t\t\t   np.mean(y_test),\n",
    "\t\t\t\t   np.mean(y_pred),\n",
    "\t\t\t\t   np.mean(errors),\n",
    "\t\t\t\t   se,\n",
    "\t\t\t\t   likelihood\n",
    "\t\t\t\t   )\n",
    "\n",
    "\t\t## plot pca\n",
    "\t\t\n",
    "\t\t## plot errors\n",
    "\n",
    "\t\tprint eval_text\n",
    "\t\treturn score\n",
    "    else:\n",
    "        return \"Please input a valid score_type {'regression', 'classification'}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRAPHICS EXPERIMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
