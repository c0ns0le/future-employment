{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import GPy\n",
    "from collections import defaultdict\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Logan/Google Drive/Oxford/DPhil/future_employment/data/helpers/skills\n"
     ]
    }
   ],
   "source": [
    "cd ../../../data/helpers/skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('skills_2009.csv')\n",
    "Y = pd.read_csv('automation_targets.csv')\n",
    "codes = pd.read_csv('codes_index.csv')\n",
    "y = pd.read_csv('automation_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "## UNSOLVED\n",
    "# Using time series data â€“ multiple years\n",
    "\n",
    "## NOTES ABOUT GP MODELS\n",
    "# optimize hyperparameters\n",
    "# perform random restarts\n",
    "# allow for multiple kernels\n",
    "# optimize kernels\n",
    "# optimize variances\n",
    "\n",
    "## TO DO\n",
    "# percentile regression model evaluation\n",
    "\n",
    "# CREATE REGRESSION MODELS\n",
    "from GPy.models import GPRegression\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from GPy.models import GPClassification\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_models = [('gpreg', GPRegression),\n",
    "\t\t\t  ('bayes_ridge', BayesianRidge()),\n",
    "\t\t\t  ('Gboost', GradientBoostingRegressor()),\n",
    "\t\t\t  ('support_vec_reg', SVR())\n",
    "\t\t\t ]\n",
    "\n",
    "# CREATE CLASSIFICATION MODELS\n",
    "class_models = [('gpclass', GPClassification),\n",
    "\t\t\t\t('ridge_class', RidgeClassifier()),\n",
    "\t\t\t\t('gbclass', GradientBoostingClassifier()),\n",
    "\t\t\t\t('support_vec_class', SVC()),\n",
    "\t\t\t\t('naivebayes', BernoulliNB())\n",
    "\t\t \t\t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CREATE INPUTS LIST\n",
    "def get_array_percentiles(array):\n",
    "\t\tdef percentile(x, array):\n",
    "\t\t\treturn 100*np.mean(array <= x)\n",
    "\n",
    "\t\treturn np.array(map(lambda x: percentile(x, array), array))\n",
    "\n",
    "def get_percentiles(df):\n",
    "\tif isinstance(df, pd.DataFrame):\n",
    "\t\tnew_df = df.copy()\n",
    "\t\treturn new_df.apply(lambda x: get_array_percentiles(x), axis = 0)\n",
    "\telif isinstance(df, np.ndarray):\n",
    "\t\treturn np.apply_along_axis(get_percentiles, 0, a)\n",
    "\telse:\n",
    "\t\tprint \"TYPE ERROR; PLEASE INPUT pd.DataFrame OR np.ndarray\"\n",
    "\t\traise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"skills_2009.csv\")\n",
    "X_skills, X_task, X_context = X, X, X\n",
    "# X_skills = pd.read_csv(\"X_skills.csv\")\n",
    "# X_task = pd.read_csv(\"X_task.csv\")\n",
    "# X_context = pd.read_csv(\"X_context.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = pd.read_csv(\"automation_targets.csv\")\n",
    "Y.columns = [\"O*NET-SOC Code\", \"auto_15\", \"auto_9\", \"delta\", \"auto_delta_pct\", \"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "codes = pd.read_csv(\"codes_index.csv\")\n",
    "full_X = pd.concat((X_skills, codes), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_X = full_X.merge(Y, on = \"O*NET-SOC Code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for dX in [full_X]:\n",
    "    pctiles = get_percentiles(dX)\n",
    "    d_list =  [('dx', dX), ('pctile', pctiles)]\n",
    "    for d in d_list:\n",
    "        prefix = d[0]\n",
    "        data = d[1]\n",
    "        inputs.append((prefix, data.iloc[:,:-6]))\n",
    "        inputs.append((prefix + '_greater_than', data[data.auto_delta_pct > 0].iloc[:,:-6]))\n",
    "        inputs.append((prefix + '_less_than', data[data.auto_delta_pct < 0].iloc[:,:-6]))\n",
    "# \t\tinputs.append(data[emp_delta_pct > 0])\n",
    "# \t\tinputs.append(data[emp_delta_pct < 0])\n",
    "\n",
    "# new_inputs = []\n",
    "# for inp in inputs:\n",
    "#     new_inputs.append(inp[0], inp[1].iloc[:,:-6])\n",
    "\n",
    "# inputs = new_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FINAL DATA\n",
    "y_out = full_X[['auto_9', 'auto_15', 'delta', 'auto_delta_pct']]\n",
    "codes = full_X[\"O*NET-SOC Code\"]\n",
    "\n",
    "full_X = full_X.drop(['O*NET-SOC Code', 'auto_9', 'auto_15', 'delta', 'auto_delta_pct', 'title'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# automation = pd.read_csv(\"y_automation.csv\")\n",
    "# computerisation = pd.read_csv(\"y_computerisation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CREATE REG TARGETS\n",
    "y_before = ('before', y_out.auto_9)\n",
    "y_after = ('after', y_out.auto_15)\n",
    "y_delta = ('delta', y_out.delta)\n",
    "y_pct = ('delta_pct', y_out.auto_delta_pct)\n",
    "reg_targets = [y_before, y_after, y_delta, y_pct]\n",
    "\n",
    "new_reg_targets = []\n",
    "for name, series in reg_targets:\n",
    "    new_reg_targets.append((name, series))\n",
    "    new_name = name + '_pctile'\n",
    "    new_series = get_array_percentiles(series)\n",
    "    new_reg_targets.append((new_name, new_series))\n",
    "    \n",
    "reg_targets = new_reg_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CREATE CLASS TARGETS\n",
    "class_targets = []\n",
    "pos_threshold_values = [0.25, 0.5, 0.75, 1.0, 1.25, 1.5, 1.75, 2.00, 2.25, 2.50, 2.75, 3.00]\n",
    "neg_threshold_values = map(lambda x: -x, pos_threshold_values)\n",
    "threshold_values = pos_threshold_values + neg_threshold_values\n",
    "\n",
    "for name, target in reg_targets:\n",
    "    for sd_threshold_value in threshold_values:\n",
    "        low_threshold = target > (np.mean(target) - sd_threshold_value*np.std(target))\n",
    "        new_name = str(sd_threshold_value)+\"_gt_\" + name\n",
    "        class_targets.append((new_name,low_threshold))\n",
    "\n",
    "# \tmean_threshold = target > np.mean(target)\n",
    "# \tnew_name = 'mean_gt_' + name\n",
    "# \tclass_targets.append((name, mean_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_inputs = inputs\n",
    "y_inputs_reg, y_inputs_class = reg_targets, class_targets\n",
    "models_reg, models_class = reg_models, class_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = [X_inputs[0]]\n",
    "y = [y_inputs_reg[4]]\n",
    "m = [('GP REG', GPRegression)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array(X_inputs[0][1])\n",
    "y = np.array(y_inputs_reg[4][1])[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = m.predict(te_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tr_x, te_x, tr_y, te_y = train_test_split(X, y)\n",
    "m = GPRegression(tr_x, tr_y)\n",
    "m.optimize()\n",
    "m.optimize_restarts(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score_model(preds, te_y, ('gpreg', X), m, 'regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### aggregate models as list of tuples [(name, val), (name, val)]\n",
    "# loop\n",
    "\n",
    "def run_loop(X_inputs, y_inputs_reg = None, y_inputs_class = None, models_reg = None, models_class = None):\n",
    "    ## returns all models in comparison, as dict\n",
    "    tree = lambda: defaultdict(tree)\n",
    "    evals = tree()\n",
    "    \n",
    "    # begin loop over inputs\n",
    "    for X_inp in X_inputs: #FOR ALL INPUTS\n",
    "        X_name, X = X_inputs[0], X_inputs[1]\n",
    "        \n",
    "        # check if null pred_type\n",
    "        if y_inputs_reg and not y_inputs_class:\n",
    "            pred_type = 'regression'\n",
    "        \n",
    "        elif y_inputs_class and not y_inputs_reg:\n",
    "            pred_type = 'classification'\n",
    "        \n",
    "        elif not(y_inputs_reg or y_inputs_class):\n",
    "            raise \"Input a target variable!\"\n",
    "        \n",
    "        else: # both are fulfilled; perform classification & regression\n",
    "            new_dict = evals.copy()\n",
    "            for pred_type in ['regression', 'classification']:\n",
    "                if pred_type == 'regression':\n",
    "                    y_inps = y_inputs_reg\n",
    "                    mods = models_reg\n",
    "                elif pred_type == 'classification':\n",
    "                    y_inputs = y_inputs_class\n",
    "                    mods = models_class\n",
    "                else:\n",
    "                    return \"SOMETHING WRONG!\"\n",
    "                new_dict.update(run_model())\n",
    "                evals = new_dict\n",
    "        \n",
    "        evals = run_model()\n",
    "                \n",
    "        def run_model(pred_type):\n",
    "            model_evals = tree()\n",
    "        \n",
    "            # do inference over all targets\n",
    "\n",
    "            for y_inp in y_inps:\n",
    "                # format outcome data\n",
    "                y_name, y = y_inp[0], y_inp[1]\n",
    "                if y.ndim == 1:\n",
    "                    y = y[:, np.newaxis]\n",
    "\n",
    "                #format feature matrix data\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "                data = [X_train, X_test, y_train, y_test]\n",
    "                new_data = []\n",
    "                # check that all data is np.ndarry\n",
    "                for d in data:\n",
    "                    if not isinstance(data, np.ndarray):\n",
    "                        new_data.append(np.array(data))\n",
    "                    else:\n",
    "                        new_data.append(data)\n",
    "                X_train, X_test, y_train, y_test = new_data\n",
    "\n",
    "                #loop over models\n",
    "                for mod in mods:\n",
    "                    model_name, model = models[0], models[1]\n",
    "                    try:\n",
    "                        model.fit(X_train, y_train)\n",
    "                    except AttributeError:\n",
    "                        model = model(X_train, y_train) # THESE ARE GPs!\n",
    "                        model.optimize()\n",
    "                        model.optimize_restarts(20)\n",
    "                    y_pred = model.predict(X_test)\n",
    "                    score = score_model(y_pred, y_test, X_inp, mod, score_type = pred_type) # CREATE TYPES\n",
    "                    model_evals[y_name][model_name][X_name] = score\n",
    "                return model_evals\n",
    "\n",
    "        return evals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_model(y_pred, y_test, X_inputs, models, score_type):\n",
    "    score = {}\n",
    "    X_name, X = X_inputs[0], X_inputs[1]\n",
    "    model_name, model = models[0], models[1]\n",
    "\n",
    "    if score_type == \"classification\":\n",
    "        accuracy = -1\n",
    "        precision = None\n",
    "        recall = None\n",
    "        specificity = None\n",
    "        f1 = None\n",
    "        ideal_cutoff = None\n",
    "\n",
    "        chart_storage = np.array([None, None, None])\n",
    "        for cutoff in np.arange(0, 1.001, 0.001):\n",
    "            y_pred = y_pred > cutoff\n",
    "            num_TP = float(np.sum((y_pred == 1) & (y_test == 1)))\n",
    "            num_FP = float(np.sum((y_pred == 1) & (y_test == 0)))\n",
    "            num_TN = float(np.sum((y_pred == 0) & (y_test == 0)))\n",
    "            num_FN = float(np.sum((y_pred == 0) & (y_test == 1)))\n",
    "\n",
    "            s_accuracy = np.mean(y_pred == y_test)\n",
    "            s_precision = num_TP / (num_TP + num_FP)\n",
    "            s_recall = num_TP / (num_TP + num_FN)\n",
    "            s_specificity = num_TN / (num_TN + num_FP)\n",
    "            s_f1 = (precision * recall) / (precision + recall)\n",
    "\n",
    "            chart_storage.append([cutoff, s_recall, s_specificity])\n",
    "\n",
    "            if s_accuracy > accuracy:\n",
    "                ideal_cutoff = cutoff\n",
    "                precision = s_precision\n",
    "                recall = s_recall\n",
    "                specificity = s_specificity \n",
    "                f1 = s_f1\n",
    "                ideal_cutoff = s_ideal_cutoff\n",
    "\n",
    "        def plot_chart(chart_storage):\n",
    "            cutoffs = chart_storage[:,0]\n",
    "            sensitivity = chart_storage[:,1]\n",
    "            specificity = chart_storage[:,2]\n",
    "            sns.plt.plot(sensitivity, specificity)\n",
    "            sns.plt.close()\n",
    "\n",
    "        eval_text = \"\"\"\n",
    "        Model \t\t| \t{}\t\t|\n",
    "        X name\t\t|\t{}\t\t|\n",
    "        Accuracy\t|\t{}\t\t|\n",
    "        Precision  \t|\t{}\t\t|\n",
    "        Recall \t\t|\t{}\t\t|\n",
    "        Specificity |\t{}\t\t|\n",
    "        F1\t\t\t|\t{}\t\t|\n",
    "        \"\"\".format(model_name,\n",
    "                   X_name,\n",
    "                   accuracy,\n",
    "                   precision,\n",
    "                   recalll,\n",
    "                   specificity,\n",
    "                   f1)\n",
    "        score['Model name'] = model_name\n",
    "        score['Model'] = model\n",
    "        score['X_name'] = X_name\n",
    "        score['Accuracy'] = accuracy\n",
    "        score['Precision'] = precision\n",
    "        score['Recall'] = recall\n",
    "        score['Specificity'] = specificity\n",
    "        score['F1'] = F1\n",
    "        score['AUC_data'] = chart_storage\n",
    "        score['chart'] = plot_chart # when called, call on plot_chart( score['AUC_data'])\n",
    "\n",
    "        print eval_text\n",
    "        return score\n",
    "\n",
    "    elif score_type == 'regression':\n",
    "        errors = y_test - y_pred\n",
    "        se = float(np.std(errors)) / np.sqrt(len(errors))\n",
    "\n",
    "        eval_text = \"\"\"\n",
    "        Model \t\t| \t{}\t\t|\n",
    "        X name\t\t|\t{}\t\t|\n",
    "        Mean Y\t\t|\t{}\t\t|\n",
    "        Mean Y_hat\t|\t{}\t\t|\n",
    "        Mean error\t|\t{}\t\t|\n",
    "        SE\t\t\t| \t{}\t\t|\n",
    "        Likelihood\t|\t##\t\t|\n",
    "        \"\"\".format(model_name,\n",
    "                   X_name,\n",
    "                   np.mean(y_test),\n",
    "                   np.mean(y_pred),\n",
    "                   np.mean(errors),\n",
    "                   se\n",
    "#                    likelihood\n",
    "                   )\n",
    "\n",
    "        ## plot pca\n",
    "\n",
    "        ## plot errors\n",
    "\n",
    "        print eval_text\n",
    "        return score\n",
    "    else:\n",
    "        return \"Please input a valid score_type {'regression', 'classification'}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_ARD(model, feature_names):\n",
    "\t# uses ARD to find relevant features\n",
    "\tlength_scales = model.length_scales # np.ndarray\n",
    "\tfeatures = range(1, len(model.features) + 1)\n",
    "\timportances = 1./length_scales\n",
    "\tcutoff = 0.25 * min(importances) # note: arbitrary\n",
    "\timportant_features = features * (importances >= cutoff)\n",
    "\tif_indices = np.trim_zeros(important_features)\n",
    "\treturn zip(feature_names[if_indices], importances[if_indices])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
